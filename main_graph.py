import os
import operator
from typing import Annotated, List, Tuple, TypedDict, Union
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from utils.logger_util import logger
from langgraph.graph import END, StateGraph, START
from pydantic import BaseModel, Field

load_dotenv()
llm = ChatOpenAI(
    model="deepseek-chat",
    api_key=os.getenv('DEEPSEEK_API_KEY'),
    base_url=os.getenv('DEEPSEEK_BASE_URL'),
    temperature=0.7,
    streaming=True  # å¼€å¯æµå¼
)

class PlanExecuteState(TypedDict):
    """å®šä¹‰çŠ¶æ€"""
    question: str # ç”¨æˆ·é—®é¢˜
    plan: List[str] # å¾…æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨
    past_steps: Annotated[List[Tuple], operator.add] # å·²å®Œæˆçš„æ­¥éª¤ï¼ˆæ­¥éª¤åï¼Œç»“æœï¼‰
    response: str # æœ€ç»ˆå›å¤

class Plan(BaseModel):
    """(ç»“æ„åŒ–è¾“å‡º) è§„åˆ’åˆ—è¡¨"""
    steps: List[str] = Field(description="ä¸€ç³»åˆ—å…·ä½“çš„æ­¥éª¤ï¼Œä¾‹å¦‚æŸ¥è¯¢å¤©æ°”ï¼ŒæŸ¥è¯¢æ™¯ç‚¹ç­‰") # è®¡åˆ’åˆ—è¡¨ç»“æ„

class Response(BaseModel):
    """ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰é‡æ–°è§„åˆ’æˆ–ç»“æŸ"""
    response: str = Field(description="æœ€ç»ˆå›ç­”ï¼Œå¦‚æœè¿˜éœ€è¦ç»§ç»­æ‰§è¡Œæ­¥éª¤ï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²")
    next_plan: List[str] = Field(description="å‰©ä½™æœªå®Œæˆçš„æ­¥éª¤åˆ—è¡¨")

def planner_node(state: PlanExecuteState):
    """æ¥æ”¶ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆåˆå§‹è®¡åˆ’"""
    logger.info("ğŸš€è§„åˆ’å¸ˆæ­£åœ¨è§„åˆ’ä»»åŠ¡")
    question = state["question"]
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ—…æ¸¸è§„åˆ’ä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ï¼Œåˆ¶å®šä¸€ä¸ªæ¸…æ™°çš„åˆ†å¸ƒæ‰§è¡Œè®¡åˆ’ã€‚"
    user_prompt = f"ç”¨æˆ·éœ€æ±‚ï¼š{question}"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # è°ƒç”¨æ¨¡å‹ï¼Œè·å–ç»“æ„åŒ–è¾“å‡º
    structured_llm  = llm.with_structured_output(Plan)
    response = structured_llm.invoke(messages)
    return {"plan": response.steps}

def executor_node(state: PlanExecuteState):
    """æ‰§è¡Œè€…ï¼šå–å‡ºè®¡åˆ’ä¸­çš„ç¬¬ä¸€ä¸ªä»»åŠ¡"""
    plan = state['plan']
    task = plan[0]

    logger.info(f"ğŸš€æ‰§è¡Œè€…æ­£åœ¨æ‰§è¡Œä»»åŠ¡ï¼š{task}")

    # 1) ç”Ÿæˆæœç´¢å…³é”®è¯
    search_query_prompt = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœç´¢åŠ©æ‰‹ï¼Œè¯·æŠŠç”¨æˆ·çš„ä»»åŠ¡è½¬æ¢ä¸ºæœ€é€‚åˆæœç´¢å¼•æ“æœç´¢çš„å…³é”®è¯ã€‚åªè¾“å‡ºå…³é”®è¯ï¼Œä¸è¦å…¶ä»–åºŸè¯ã€‚"},
        {"role": "user", "content": f"ä»»åŠ¡ï¼š{task}"}
    ]
    keywords_text = llm.invoke(search_query_prompt)
    search_query = keywords_text.content.strip()
    logger.info(f"æœç´¢å…³é”®è¯ï¼š{search_query}")

    # 2ï¼‰è°ƒç”¨ Tavilyå·¥å…·
    try:
        search_result = tavily_tool.invoke(search_query)
        result_str = json.dumps(search_result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥ï¼š{e}")
        return {"response": f"æœç´¢å¤±è´¥ï¼š{e}"}

    logger.info(f"æœç´¢ç»“æœé•¿åº¦ä¸ºï¼š{len(result_str)}")

    return {
        "past_steps": [(task, result_str)]
    }

def replanner_node(state: PlanExecuteState):
    """é‡æ–°è§„åˆ’å™¨ï¼šæ ¹æ®æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’"""
    logger.info(f"ğŸš€é‡æ–°è§„åˆ’å¸ˆæ­£åœ¨åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’")
    past_steps_str = ""
    for step, result in state['past_steps']:
        past_steps_str += f"å·²å®Œæˆæ­¥éª¤ï¼š{step}\næ‰§è¡Œç»“æœï¼š{result}\n"
    current_plan_str = "\n".join(state['plan'])

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è°ƒåº¦ç³»ç»Ÿã€‚\n"
        "1. æ£€æŸ¥'å·²å®Œæˆæ­¥éª¤'çš„ä¿¡æ¯æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·çš„'åŸå§‹ç›®æ ‡'ã€‚\n"
        "2. å¦‚æœè¶³å¤Ÿï¼Œè¯·åœ¨responseå­—æ®µä¸­è¾“å‡ºæœ€ç»ˆçš„å›ç­”ï¼ˆMarkdown æ ¼å¼ï¼‰ï¼Œå¹¶å°†new_planè®¾ä¸ºç©ºåˆ—è¡¨ã€‚\n"
        "3. å¦‚æœä¸è¶³å¤Ÿï¼Œè¯·æ ¹æ®æ‰§è¡Œç»“æœæ›´æ–°å‰©ä½™çš„è®¡åˆ’ï¼ˆå»æ‰å·²å®Œæˆçš„ï¼Œæˆ–è€…æ·»åŠ æ–°çš„æ­¥éª¤ï¼‰ï¼Œå¡«å…¥new_planå­—æ®µã€‚"
    )

    user_prompt = (
        f"åŸå§‹ç›®æ ‡ï¼š{state['question']}\n"
        f"å·²å®Œæˆæ­¥éª¤ï¼š{past_steps_str}\n"
        f"å½“å‰è®¡åˆ’ï¼š{current_plan_str}\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    structured_llm = llm.with_structured_output(Response)
    result = structured_llm.invoke(messages)

    if result.response and result.response.strip() != "":
        logger.info("ä»»åŠ¡å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚")
        return {"response": result.response, "plan": []}
    else:
        logger.info(f"é‡æ–°è§„åˆ’å¸ˆå†³ç­–ï¼šç»§ç»­æ‰§è¡Œï¼Œå‰©ä½™è®¡åˆ’ï¼š{len(result.next_plan)}ä¸ªæ­¥éª¤")
        return {"plan": result.next_plan}

def should_end(state: PlanExecuteState):
    """åˆ¤æ–­æµç¨‹æ˜¯å¦éœ€è¦ç»“æŸ"""
    if state['response']:
        return True
    else:
        return False

workflow = StateGraph(PlanExecuteState)

workflow.add_node("planner", planner_node)
workflow.add_node("executor", executor_node)
workflow.add_node("replanner", replanner_node)

workflow.add_edge(START, "planner")         # å¼€å§‹ -> è§„åˆ’
workflow.add_edge("planner", "executor")    # è§„åˆ’ -> æ‰§è¡Œè€…
workflow.add_edge("executor", "replanner")  # æ‰§è¡Œè€… -> åæ€

# æ·»åŠ æ¡ä»¶åˆ†æ”¯
workflow.add_conditional_edges(
    "replanner", # ä»åæ€èŠ‚ç‚¹å‡ºæ¥
    should_end, # åˆ¤æ–­æ˜¯å¦ç»“æŸ
    {
        True: END, # å¦‚æœè¿”å› Trueï¼Œæµç¨‹ç»“æŸ
        False: "executor" # å¦‚æœè¿”å› Falseï¼Œç»§ç»­æ‰§è¡Œ
    }
)

app = workflow.compile()

if __name__ == "__main__":
    question = "æˆ‘æƒ³å»æ´›é˜³ç©ç©ï¼Œå¸®æˆ‘æŸ¥æŸ¥é¾™é—¨çŸ³çªŸæ˜å¤©çš„å¤©æ°”ï¼Œä»¥åŠé—¨ç¥¨ä»·æ ¼ã€‚"
    state = {"question": question}

    for event in app.stream(state):
        # eventæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkeyæ˜¯èŠ‚ç‚¹åç§°ï¼Œvalueæ˜¯è¯¥èŠ‚ç‚¹è¾“å‡ºçš„state
        for node_name, node_state in event.items():
            # å› ä¸ºå·²ç»åœ¨èŠ‚ç‚¹ä¸­å¤„ç†äº†æ—¥å¿—ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤æ‰“å°
            pass

    # è·å–æœ€ç»ˆå›ç­”
    final_response = node_state['response']
    logger.info(f"æœ€ç»ˆå›ç­”ï¼š{final_response}")

import json
import operator
import os
from typing import Annotated, List, Tuple, TypedDict

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_tavily import TavilySearch
from langgraph.graph import END, StateGraph, START
from pydantic import BaseModel, Field
import uuid

from utils.logger_util import logger
from utils.parse_llm_json_util import parse_llm_json
from prompts import route_prompt, direct_answer_prompt

from langgraph.checkpoint.postgres import PostgresSaver
from psycopg_pool import ConnectionPool

load_dotenv()
llm = ChatOpenAI(
    model="deepseek-chat",
    api_key=os.getenv('DEEPSEEK_API_KEY'),
    base_url=os.getenv('DEEPSEEK_BASE_URL'),
    temperature=0.7,
    streaming=True  # å¼€å¯æµå¼
)
tavily_tool = TavilySearch(max_results=5)


class PlanExecuteState(TypedDict):
    """å®šä¹‰çŠ¶æ€"""
    question: str  # ç”¨æˆ·é—®é¢˜
    plan: List[str]  # å¾…æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨
    past_steps: Annotated[List[Tuple], operator.add]  # å·²å®Œæˆçš„æ­¥éª¤ï¼ˆæ­¥éª¤åï¼Œç»“æœï¼‰
    response: str  # æœ€ç»ˆå›å¤
    route: str # è·¯ç”±æ„å›¾


class Plan(BaseModel):
    """(ç»“æ„åŒ–è¾“å‡º) è§„åˆ’åˆ—è¡¨"""
    steps: List[str] = Field(description="ä¸€ç³»åˆ—å…·ä½“çš„æ­¥éª¤ï¼Œä¾‹å¦‚æŸ¥è¯¢å¤©æ°”ï¼ŒæŸ¥è¯¢æ™¯ç‚¹ç­‰")  # è®¡åˆ’åˆ—è¡¨ç»“æ„


class Response(BaseModel):
    """ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰é‡æ–°è§„åˆ’æˆ–ç»“æŸ"""
    response: str = Field(description="æœ€ç»ˆå›ç­”ï¼Œå¦‚æœè¿˜éœ€è¦ç»§ç»­æ‰§è¡Œæ­¥éª¤ï¼Œåˆ™ä¸ºç©ºå­—ç¬¦ä¸²")
    next_plan: List[str] = Field(description="å‰©ä½™æœªå®Œæˆçš„æ­¥éª¤åˆ—è¡¨")

def router_node(state: PlanExecuteState):
    """è·¯ç”±èŠ‚ç‚¹ï¼šåˆ¤æ–­æ„å›¾"""
    logger.info("ğŸš€è·¯ç”±å¸ˆæ­£åœ¨åˆ¤æ–­æ„å›¾")
    question = state["question"]

    prompt = route_prompt.format(user_request=question)
    raw = llm.invoke(prompt)
    try:
        data = parse_llm_json(raw.content)
        route = str(data.get("route", "")).strip()
    except Exception as e:
        logger.error(f"è·¯ç”±è§£æå¤±è´¥ï¼š{e}")
        route = ""

    if route not in {"planner", "direct_answer"}:
        logger.info(f"è·¯ç”±ç»“æœæ— æ•ˆï¼Œé»˜è®¤èµ° planner: {route}")
        route = "planner"

    logger.info(f"ç”¨æˆ·æ„å›¾ï¼š{route}")
    return {"route": route}


def direct_answer_node(state: PlanExecuteState):
    """ç›´æ¥å›ç­”ï¼šæ— éœ€å·¥å…·"""
    logger.info("ğŸš€ç›´æ¥å›ç­”ä¸­")
    question = state["question"]
    prompt = direct_answer_prompt.format(user_request=question)
    raw = llm.invoke(prompt)
    return {"response": raw.content}


def planner_node(state: PlanExecuteState):
    """æ¥æ”¶ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆåˆå§‹è®¡åˆ’"""
    logger.info("ğŸš€è§„åˆ’å¸ˆæ­£åœ¨è§„åˆ’ä»»åŠ¡")
    question = state["question"]

    # å¦‚æœæ˜¯å¤šè½®å¯¹è¯ï¼Œpast_stepså…¶ä¸­ä¼šæœ‰ä¹‹å‰çš„æ‰§è¡Œè®°å½•
    past_steps_context = ""
    if state.get("past_steps"):
        past_info = "\n".join([f"æ­¥éª¤ï¼š{step}ï¼Œç»“æœæ‘˜è¦ï¼š{res[:50]}..." for step, res in state["past_steps"]])
        past_steps_context = f"\n\nå·²çŸ¥å†å²ä¿¡æ¯ï¼ˆä¸ç”¨é‡å¤æŸ¥è¯¢ï¼‰ï¼š\n{past_info}"

    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ—…æ¸¸è§„åˆ’ä¸“å®¶ã€‚ä»…è¾“å‡º JSONã€‚å­—æ®µï¼šsteps(string[])ã€‚ä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–è§£é‡Šã€‚"
    user_prompt = f"ç”¨æˆ·éœ€æ±‚ï¼š{question}{past_steps_context}"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    raw = llm.invoke(messages)
    try:
        data = parse_llm_json(raw.content)
        parsed = Plan.model_validate(data)
        steps = parsed.steps
        logger.info(f"è§„åˆ’ç»“æœï¼š{steps}")
    except Exception as e:
        logger.error(f"è§„åˆ’è§£æå¤±è´¥ï¼š{e}")
        steps = []
    return {"plan": steps}


def executor_node(state: PlanExecuteState):
    """æ‰§è¡Œè€…ï¼šå–å‡ºè®¡åˆ’ä¸­çš„ç¬¬ä¸€ä¸ªä»»åŠ¡"""
    plan = state['plan']
    if not plan:
        logger.error("è®¡åˆ’ä¸ºç©º")
        return {"past_steps": [], "response": ""}
    task = plan[0]

    logger.info(f"ğŸš€æ‰§è¡Œè€…æ­£åœ¨æ‰§è¡Œä»»åŠ¡ï¼š{task}")

    # 1) ç”Ÿæˆæœç´¢å…³é”®è¯
    search_query_prompt = [
        {"role": "system",
         "content": "ä½ æ˜¯ä¸€ä¸ªæœç´¢åŠ©æ‰‹ï¼Œè¯·æŠŠç”¨æˆ·çš„ä»»åŠ¡è½¬æ¢ä¸ºæœ€é€‚åˆæœç´¢å¼•æ“æœç´¢çš„å…³é”®è¯ã€‚åªè¾“å‡ºå…³é”®è¯ï¼Œä¸è¦å…¶ä»–åºŸè¯ã€‚"},
        {"role": "user", "content": f"ä»»åŠ¡ï¼š{task}"}
    ]
    keywords_text = llm.invoke(search_query_prompt)
    search_query = keywords_text.content.strip()
    logger.info(f"æœç´¢å…³é”®è¯ï¼š{search_query}")

    # 2ï¼‰è°ƒç”¨ Tavilyå·¥å…·
    try:
        search_result = tavily_tool.invoke(search_query)
        result_str = json.dumps(search_result, ensure_ascii=False)
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥ï¼š{e}")
        return {"response": f"æœç´¢å¤±è´¥ï¼š{e}"}

    logger.info(f"æœç´¢ç»“æœé•¿åº¦ä¸ºï¼š{len(result_str)}")

    return {
        "past_steps": [(task, result_str)],
        "plan": plan[1:] # å‰”é™¤ç¬¬ä¸€ä¸ªä»»åŠ¡
    }


def reflect_node(state: PlanExecuteState):
    """é‡æ–°è§„åˆ’å™¨ï¼šæ ¹æ®æ‰§è¡Œç»“æœï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’"""
    logger.info(f"ğŸš€é‡æ–°è§„åˆ’å¸ˆæ­£åœ¨åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’")
    past_steps_str = ""
    for step, result in state['past_steps']:
        past_steps_str += f"å·²å®Œæˆæ­¥éª¤ï¼š{step}\næ‰§è¡Œç»“æœï¼š{result}\n"

    current_plan_str = "\n".join(state['plan'])

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è°ƒåº¦ç³»ç»Ÿã€‚ä»…è¾“å‡º JSONã€‚å­—æ®µï¼šresponse(string)ã€next_plan(string[])ã€‚\n"
        "å½“ä¿¡æ¯è¶³å¤Ÿæ—¶ï¼Œå°† next_plan è®¾ä¸ºç©ºæ•°ç»„ï¼Œå¹¶åœ¨ response ä¸­ç»™å‡ºæœ€ç»ˆ Markdown å›ç­”ï¼›\n"
        "å½“ä¿¡æ¯ä¸è¶³æ—¶ï¼Œresponse è®¾ä¸ºç©ºå­—ç¬¦ä¸²ã€‚ä¼˜å…ˆä¿ç•™å½“å‰è®¡åˆ’ï¼Œåªåœ¨å¿…è¦æ—¶è°ƒæ•´ã€‚\n"
        "å¦‚éœ€ç»§ç»­æ‰§è¡Œï¼Œnext_plan åº”å°½é‡ç­‰äºå½“å‰è®¡åˆ’ä¸­å°šæœªå®Œæˆçš„éƒ¨åˆ†ï¼›\n"
        "åªæœ‰åœ¨ç°æœ‰æ­¥éª¤æ˜æ˜¾é”™è¯¯æˆ–ç¼ºå°‘å…³é”®æ­¥éª¤æ—¶æ‰å…è®¸ä¿®æ”¹ï¼Œå¹¶ä¸”æœ€å¤šæ–°å¢ 1-2 ä¸ªæ­¥éª¤ã€‚\n"
        "ä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬æˆ–è§£é‡Šã€‚"
    )

    user_prompt = (
        f"åŸå§‹ç›®æ ‡ï¼š{state['question']}\n"
        f"å†å²ï¼š{past_steps_str}\n"
        f"å½“å‰è®¡åˆ’ï¼š{current_plan_str}\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    raw = llm.invoke(messages)
    try:
        data = parse_llm_json(raw.content)
        result = Response.model_validate(data)
    except Exception as e:
        logger.error(f"é‡æ–°è§„åˆ’è§£æå¤±è´¥ï¼š{e}")
        result = Response(response="", next_plan=[])

    if result.response and result.response.strip() != "":
        logger.info("ä»»åŠ¡å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚")
        return {"response": result.response, "plan": []}
    else:
        logger.info(f"é‡æ–°è§„åˆ’å¸ˆå†³ç­–ï¼šç»§ç»­æ‰§è¡Œï¼Œå‰©ä½™è®¡åˆ’ï¼š{len(result.next_plan)}ä¸ªæ­¥éª¤")
        logger.info(f"å‰©ä½™è®¡åˆ’ï¼š{result.next_plan}")
        return {"plan": result.next_plan}


def route_by_intent(state: PlanExecuteState):
    route = state.get("route")
    return route if route in {"planner", "direct_answer"} else "planner"


def should_end(state: PlanExecuteState):
    """åˆ¤æ–­æµç¨‹æ˜¯å¦éœ€è¦ç»“æŸ"""
    if state.get('response'):
        return True
    else:
        return False


workflow = StateGraph(PlanExecuteState)

workflow.add_node("router", router_node)
workflow.add_node("planner", planner_node)
workflow.add_node("executor", executor_node)
workflow.add_node("reflect", reflect_node)
workflow.add_node("direct_answer", direct_answer_node)

workflow.add_edge(START, "router")
workflow.add_conditional_edges(
    "router", # è·¯ç”±èŠ‚ç‚¹æ‰§è¡Œå®Œï¼Œè¿›è¡Œåˆ¤æ–­
    route_by_intent, # åˆ¤æ–­å‡½æ•°
    {
        "planner": "planner", # å‡½æ•°çš„è¿”å›å€¼æ˜¯plannerï¼Œåˆ™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯planner
        "direct_answer": "direct_answer"
    }
)
workflow.add_edge("direct_answer", END)
workflow.add_edge("planner", "executor")  # è§„åˆ’ -> æ‰§è¡Œè€…
workflow.add_edge("executor", "reflect")  # æ‰§è¡Œè€… -> åæ€

# æ·»åŠ æ¡ä»¶åˆ†æ”¯
workflow.add_conditional_edges(
    "reflect",  # ä»åæ€èŠ‚ç‚¹å‡ºæ¥
    should_end,  # åˆ¤æ–­æ˜¯å¦ç»“æŸ
    {
        True: END,  # å¦‚æœè¿”å› Trueï¼Œæµç¨‹ç»“æŸ
        False: "executor"  # å¦‚æœè¿”å› Falseï¼Œç»§ç»­æ‰§è¡Œ
    }
)



if __name__ == "__main__":
    uuid = uuid.uuid4().hex
    DB_URI = os.getenv("POSTGRES_URI")
    with ConnectionPool(DB_URI) as pool:
        # 1) åˆå§‹åŒ–PgSaver
        checkpointer = PostgresSaver(pool)

        # 2) é¦–æ¬¡è¿è¡Œï¼Œå¿…é¡»æ‰§è¡Œ setup()ï¼Œå®ƒä¼šè‡ªåŠ¨åœ¨åº“é‡Œåˆ›å»ºä¸¤å¼ è¡¨ï¼ˆcheckpointsã€checkpoint_writesï¼‰
        checkpointer.setup()

        app = workflow.compile(checkpointer=checkpointer)

        config = {"configurable": {"thread_id": uuid}}

        # è¿è¡Œç¬¬ä¸€è½®
        question = "ç‰¹æœ—æ™®å¤šå°‘å²äº†"
        state = {"question": question}
        logger.info("ç¬¬ä¸€è½®è¿è¡Œå¼€å§‹")
        for event in app.stream(state, config=config):
            pass
        # è¾“å‡ºæœ€ç»ˆå›ç­”
        final_state = app.get_state(config)
        final_response = final_state.values.get('response', '')
        logger.info(f"é—®é¢˜ï¼š{question}")
        logger.info(f"æœ€ç»ˆå›ç­”ï¼š{final_response}")

        # è¿è¡Œç¬¬äºŒè½®ï¼ˆæµ‹è¯•è®°å¿†ï¼‰
        logger.info("ç¬¬äºŒè½®è¿è¡Œå¼€å§‹")
        new_question = "åˆšæ‰è¯´çš„å°è¡—å¤©åºœï¼Œæœ‰ä»€ä¹ˆå¥½åƒçš„"
        app.update_state(config, {"question": new_question, "response": ""})
        # ä¼ å…¥Noneï¼Œè¡¨ç¤ºå»¶ç»­çŠ¶æ€
        for event in app.stream(None, config=config):
            pass
        # è¾“å‡ºæœ€ç»ˆå›ç­”
        final_state = app.get_state(config)
        final_response = final_state.values.get('response', '')
        logger.info(f"é—®é¢˜ï¼š{question}")
        logger.info(f"æœ€ç»ˆå›ç­”ï¼š{final_response}")
